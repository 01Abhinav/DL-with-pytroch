{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(root='data/',download = True ,transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = MNIST(root='data/' , train=False , transform = transforms.ToTensor())\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) lebel: 1\n"
     ]
    }
   ],
   "source": [
    "img_tensor , lebel =dataset[809]\n",
    "\n",
    "print(img_tensor.shape,\"lebel:\",lebel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f46fcb48b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_tensor[0,10:15,12:21] , cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds , val_ds = random_split(dataset,[50000,10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "train_loader = DataLoader(train_ds,batch_size,shuffle = True)\n",
    "val_loader = DataLoader(val_ds, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28\n",
    "num_class = 10\n",
    "\n",
    "model = nn.Linear(input_size , num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_class)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0129, -0.0020,  0.0034,  ...,  0.0324, -0.0104,  0.0248],\n",
       "         [-0.0304,  0.0067, -0.0110,  ...,  0.0324, -0.0226, -0.0287],\n",
       "         [ 0.0146,  0.0260,  0.0185,  ...,  0.0241, -0.0336, -0.0306],\n",
       "         ...,\n",
       "         [ 0.0325,  0.0053,  0.0336,  ...,  0.0013,  0.0238, -0.0349],\n",
       "         [-0.0076,  0.0041, -0.0258,  ...,  0.0013, -0.0288, -0.0318],\n",
       "         [ 0.0083, -0.0288,  0.0295,  ..., -0.0015,  0.0032,  0.0173]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0262, -0.0079,  0.0211,  0.0233, -0.0127, -0.0125, -0.0326, -0.0217,\n",
       "         -0.0049, -0.0245], requires_grad=True)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=10, bias=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28]) tensor([9, 0, 3, 6, 8, 6, 2, 1, 1, 2, 2, 6, 1, 2, 3, 6, 2, 2, 7, 0, 1, 0, 8, 1,\n",
      "        2, 1, 1, 6, 0, 2, 7, 1, 3, 6, 0, 7, 8, 1, 4, 8, 0, 2, 3, 3, 5, 4, 1, 6,\n",
      "        6, 6, 9, 5, 0, 9, 7, 6, 5, 5, 5, 8, 4, 9, 4, 5, 1, 5, 1, 6, 8, 7, 8, 1,\n",
      "        0, 5, 2, 6, 5, 5, 5, 0, 1, 9, 9, 5, 4, 2, 6, 0, 2, 9, 3, 9, 1, 9, 2, 4,\n",
      "        8, 6, 2, 7, 4, 3, 9, 5, 2, 2, 6, 9, 9, 7, 1, 5, 1, 1, 1, 3, 4, 1, 8, 8,\n",
      "        3, 7, 8, 8, 3, 4, 4, 0])\n",
      "torch.Size([128, 10])\n"
     ]
    }
   ],
   "source": [
    "for images , labels in train_loader:\n",
    "    print(images.shape , labels)\n",
    "    output = model(images)\n",
    "    print(output.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = F.softmax(output, dim=1)\n",
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prob,pred = torch.max(prob ,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0569, -0.0964,  0.0769,  ...,  0.2511,  0.0152, -0.3658],\n",
       "        [ 0.0443, -0.0931,  0.2875,  ..., -0.0668, -0.0603, -0.1754],\n",
       "        [-0.1746, -0.1087, -0.0199,  ..., -0.1482, -0.1189, -0.0243],\n",
       "        ...,\n",
       "        [-0.1979, -0.1123,  0.0287,  ..., -0.0816,  0.1945, -0.0707],\n",
       "        [-0.0048, -0.0161,  0.2647,  ..., -0.1581, -0.0571, -0.2023],\n",
       "        [ 0.1807,  0.1086, -0.0197,  ...,  0.4973,  0.0772, -0.2166]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output , labels):\n",
    "    _,preds = torch.max(output,dim=1)\n",
    "    return torch.sum(pred == labels).item()/len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0625"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(output , labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3503, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_function(output , labels)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    history = [] # for recording epoch-wise results\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size,num_class)\n",
    "    \n",
    "    def forward(self,xb):\n",
    "        xb=xb.reshape(-1,784)\n",
    "        out=self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self,batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out,labels)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self,batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        acc = accuracy(out, labels)\n",
    "        return {'val_loss':loss,'val_acc':acc}\n",
    "    \n",
    "    def validation_epoch_end(self,outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()  \n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()     \n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "    \n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "result0 = evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MNIST(root='data/', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img,model):\n",
    "    xb = img.unsqueeze(0)\n",
    "    yb =  model(xb)\n",
    "    _,preds = torch.max(yb, dim=1)\n",
    "    return preds[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 0  predicted: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[3]\n",
    "plt.imshow(img[0],cmap='gray')\n",
    "print('label:',label,' predicted:', predict_image(img,model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
